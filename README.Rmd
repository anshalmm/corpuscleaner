---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  message = F,
  warning = F
)
```

# corpuscleaner

<!-- badges: start -->
<!-- badges: end -->

## Overview 
The `{corpuscleaner}` package has a collection of `18 Documents` cleaned and ready for use regarding Text Analysis. It has a dataset `Documents` which has the collection consisting of: 

* `ID`: ID of the Document
* `title`: Title of the Document
* `text`: Text of the Document 
* `year`: Year the Document was Published 
* `author`: Author of the Document

Along with this, the package has a function `novels` which has a one row per line format for the `text`, along with the same variables as listed above. 

It also has a function `document_by_ID(..., vars)` that has a `tibble` consisting of the Document's: 

* `id`
* `text` 

and can add any `column_names` of your choice such as the `year`, `author`, or both.

## Installation

You can install the development version of corpuscleaner from [GitHub](https://github.com/) with:

``` r
# install.packages("corpuscleaner")
devtools::install_github("anshalmm/corpuscleaner")
```

## Examples

These are basic examples of what `corpuscleaner` can do when combined with basic text analysis:

```{r example}
library(corpuscleaner)
library(dplyr)
library(tidytext)

novels() %>%
  unnest_tokens(word, 
                text, 
                token = "words") %>%
  anti_join(get_stopwords("en", source = "smart"))
```

Here, we can use the `document_by_ID` function to extract the `id`, `text`, and an additional column, `author`, for the document `Vathek`.

Additionally, we can do some `frequency list` text analysis on this document by finding all the unigrams in it:

```{r}
Documents
document_by_ID(id == 1, vars = "year")
unigram_Analysis = document_by_ID(id == 1, vars = "year")

UA = unigram_Analysis %>%
  unnest_tokens(word, 
                text, 
                token = "words") %>%
  anti_join(get_stopwords("en", source = "smart")) 

UA_Count_Words = UA %>%
  count(word, sort = T)
UA_Count_Words
```


If you would like to know more about this package, please see the [Get started](https://anshalmm.github.io/corpuscleaner/articles/corpuscleaner.html) Page.  
