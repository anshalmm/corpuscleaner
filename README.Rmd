---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%",
  message = F,
  warning = F
)
```

# corpuscleaner

<!-- badges: start -->
<!-- badges: end -->

## Overview 
The `{corpuscleaner}` package has a collection of `18 Documents` cleaned and ready for use regarding Text Analysis. It has a dataset `Documents` which has the collection consisting of: 

* `ID`: ID of the Document
* `title`: Title of the Document
* `text`: Text of the Document 
* `year`: Year the Document was Published 
* `author`: Author of the Document


It also has functions relating to `text analysis` such as:

* `novels` which has a one row per line format for the `text`, along with the same variables as listed above.
* `document_by_ID(..., vars)` that has a `tibble` consisting of the Document's: 

* `id`
* `text` 

and can add any `column_names` of your choice such as the `year`, `author`, or both.

Alongside this, there are `Frequency List` functions relative to the `Documents` dataset : 

* `unnest_unigrams`
* `unnest_bigrams`
* `unnest_trigrams`
* `unnest_N4_grams`
* `unnest_N5_grams`


## Installation

You can install the development version of corpuscleaner from [GitHub](https://github.com/) with:

``` r
# install.packages("corpuscleaner")
devtools::install_github("anshalmm/corpuscleaner")
```

## Usage

```{r example}
library(corpuscleaner)
library(dplyr)
library(tidytext)
# novels() 
novels() %>%
  unnest_tokens(word, 
                text, 
                token = "words") %>%
  anti_join(get_stopwords("en", source = "smart"))

# document_by_ID
Documents
document_by_ID(id == 1, vars = "year")

# Frequency List
unigram_Analysis = unnest_unigrams(id == 4)
UA = unigram_Analysis %>%
  anti_join(get_stopwords("en", source = "smart"))
UA
```

If you would like to know more about this package, please see the [Get started](https://anshalmm.github.io/corpuscleaner/articles/corpuscleaner.html) Page.  
