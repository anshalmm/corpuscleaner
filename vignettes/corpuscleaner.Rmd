---
title: "Introduction of corpuscleaner"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction of corpuscleaner}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>", 
  message = F,
  warning = F
)
```

```{r packages-used}
library(corpuscleaner)
library(dplyr)
library(tidytext)
library(magrittr)
library(topicmodels)
library(stringr)
library(tidyr)
library(reshape2)
```

# Understanding corpuscleaner

The `{corpuscleaner}` package has two datasets called `Documents` and `novels_tgt`, which has a one row per line format for the text. It consists of the same five variables each:

-   `ID`: ID of the Document
-   `title`: Title of the Document
-   `text`: Text of the Document
-   `year`: Year the Document was Published
-   `author`: Author of the Document

It also includes two functions relative to Text Analysis:

-   `novels()` which has the 18 Document collection with one row per line of text along with the same variables as above .
-   `document_by_ID` which minimizes using the two datasets for text analysis, and instead pick, by `id`, the document of your choice and an additional column name, such as the `year` the document was published.

And Frequency Lists: 

* `unnest_unigrams` 
* `unnest_bigrams` 
* `unnest_trigrams` 
* `unnest_N4_grams` 
* `unnest_N5_grams`

These are examples of basic Text Analysis of the datasets using the `novels()` and `document_by_ID` functions.

```{r}
# novels()
novels() %>%
  unnest_tokens(word, 
                text, 
                token = "words") %>%
  anti_join(get_stopwords("en", source = "smart")) %>%
  count(ID, title, word, sort = T)

# document_by_ID
Documents
document_by_ID(id == 1, vars = "year") 
```

# Frequency Lists
You can find frequency lists using the `Frequency List` functions

```{r}
# Unigram Analysis
# Load the Documents dataset 
corpuscleaner::Documents
UAnalysis = unnest_unigrams(id == 7)
UWords = UAnalysis %>%
  anti_join(get_stopwords("en", source = "smart"))
UWords

UCount = UWords %>%
  count(word, sort = T)
UCount

# Bigram Analysis
BAnalysis = unnest_bigrams(id == 4)
BWords = BAnalysis %>%
  separate(token_two, c("word1", "word2"), sep = " ") %>%
  filter(!(word1 %in% stopwords::stopwords("en", source = "smart"))) %>%
  filter(!(word2 %in% stopwords::stopwords("en", source = "smart"))) %>%
  unite(token_two, word1, word2, sep = " ")
BWords

BCount = BWords %>%
  count(token_two, sort = T)
BCount
```

# Usage in Text Analysis

Aside from this, you can do higher text analysis using `novels` such as `tf-idf`:

```{r}
# Part 1: Finding the commonly used words in all documents
doc_words = novels() %>%
  unnest_tokens(word, text) %>%
  count(title, word, sort = T)

tot_words = doc_words %>%
  group_by(title) %>% 
  summarize(total = sum(n))

put_words = left_join(doc_words, tot_words)
put_words

# Part 2: Using Zipf's law in Finding Word Frequency
frequency_rank = put_words %>%
  group_by(title) %>%
  mutate(rank = row_number(),
         term_frequency = n/total) %>%
  ungroup()

frequency_rank
```

Or do some `Topic Modeling`:

```{r}
# Using novels to do some Topic Modeling
# Part 1: Find the books you want to use in Topic Modeling 
# Search for Novels that have CHAPTER I... In Them 
novels() %>%
  filter(str_detect(text, "^chapter ")) %>%
  select(title, text)

# Optional: 
# novels() %>%
#   filter(str_detect(text, "^chapter ")) %>%
#   select(title, text) %>%
#   print(n = 250)

book_titles = c("Frankenstein", "Ivanhoe", "The Portrait of a Lady", "Wuthering Heights")

books = novels() %>%
  filter(title %in% book_titles)

# Diving these books by chapter 
book_chapters = books %>%
  group_by(title) %>%
  mutate(chapter = cumsum(str_detect(text, "^chapter "))) %>%
  filter(chapter > 0) %>%
  unite(document, title, chapter)

book_words = book_chapters %>%
  unnest_tokens(word, text) 

book_wordcounts = book_words %>%
  anti_join(get_stopwords("en", source = "smart")) %>%
  count(document, word, sort = T)

book_wordcounts

# Part 2: Convert to LDA for Topic Modeling
book_chapter_DTM = book_wordcounts %>%
  cast_dtm(document, word, n)

book_chapter_DTM

# Use LDA in Four Topic Modeling given we have Four Documents
book_chapter_LDA = LDA(book_chapter_DTM, k = 4, control = list(seed = 1234))
book_chapter_LDA

# Finally, examine a per topic per word probabilities 
book_topics = tidy(book_chapter_LDA, matrix = "beta")
book_topics
```

Therefore, this package is useful in higher or basic Text Analysis.
